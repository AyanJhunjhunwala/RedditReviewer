{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from transformers import pipeline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_reddit_topics(search_query):\n",
    "    # Use search endpoint with query\n",
    "    url = f\"https://www.reddit.com/search.json?q={search_query}&sort=relevance\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.3'\n",
    "    }\n",
    "    try:\n",
    "        # Get search results\n",
    "        response = requests.get(url, headers=headers)\n",
    "        data = response.json()\n",
    "        \n",
    "        posts_with_comments = []\n",
    "        \n",
    "        for post in data['data']['children']:\n",
    "            post_data = post['data']\n",
    "            \n",
    "            # Get comments for this post\n",
    "            comments_url = f\"https://www.reddit.com{post_data['permalink']}.json\"\n",
    "            comments_response = requests.get(comments_url, headers=headers)\n",
    "            comments_data = comments_response.json()\n",
    "            \n",
    "            # Extract comments\n",
    "            comments = []\n",
    "            if len(comments_data) > 1:  # Check if there are comments\n",
    "                for comment in comments_data[1]['data']['children']:\n",
    "                    if 'body' in comment['data']:\n",
    "                        comments.append({\n",
    "                            'author': comment['data'].get('author', '[deleted]'),\n",
    "                            'body': comment['data']['body'],\n",
    "                            'score': comment['data'].get('score', 0),\n",
    "                            'created_utc': datetime.fromtimestamp(comment['data']['created_utc']).isoformat()\n",
    "                        })\n",
    "            \n",
    "            # Combine post and comments data\n",
    "            posts_with_comments.append({\n",
    "                'title': post_data['title'],\n",
    "                'author': post_data['author'],\n",
    "                'score': post_data['score'],\n",
    "                'url': post_data['url'],\n",
    "                'created_utc': datetime.fromtimestamp(post_data['created_utc']).isoformat(),\n",
    "                'num_comments': post_data['num_comments'],\n",
    "                'selftext': post_data.get('selftext', ''),\n",
    "                'comments': comments\n",
    "            })\n",
    "        \n",
    "        # Save to JSON file\n",
    "            \n",
    "        return posts_with_comments\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! Welcome to the Reddit Web Scraper!\n"
     ]
    }
   ],
   "source": [
    "#webscraping particular subreddits\n",
    "print(\"Hi! Welcome to the Reddit Web Scraper!\")\n",
    "subreddit = input(\"Please enter the subreddit you would like to scrape: \")\n",
    "posts = scrape_reddit_topics(subreddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraped data is in JSON format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_posts_and_comments(json_obj):\n",
    "    texts = []\n",
    "    for entry in json_obj:\n",
    "        # Extract post content (selftext)\n",
    "        if 'selftext' in entry and entry['selftext']:\n",
    "            texts.append(entry['selftext'])\n",
    "        \n",
    "        # Extract comments\n",
    "        if 'comments' in entry:\n",
    "            for comment in entry['comments']:\n",
    "                if 'body' in comment:\n",
    "                    texts.append(comment['body'])\n",
    "    return texts\n",
    "\n",
    "posts_and_comments = extract_posts_and_comments(posts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tf_keras/src/initializers/initializers.py:121: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "Some layers from the model checkpoint at allenai/longformer-base-4096 were not used when initializing TFLongformerForSequenceClassification: ['lm_head']\n",
      "- This IS expected if you are initializing TFLongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFLongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFLongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use 0\n"
     ]
    }
   ],
   "source": [
    "#NLP stuff \n",
    "classifier = pipeline('sentiment-analysis', model='allenai/longformer-base-4096')\n",
    "\n",
    "results=[]\n",
    "for text in posts_and_comments:\n",
    "    result = classifier(text)\n",
    "    results.append(result)\n",
    "\n",
    "#label 0: negative, label 1: positive\n",
    "negative_count = 0\n",
    "positive_count = 0\n",
    "for count in results:\n",
    "    if count[0]['label'] == 'LABEL_1':\n",
    "        negative_count += 1\n",
    "    elif count[0]['label'] == 'LABEL_0':\n",
    "        positive_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualizing the NLP data\n",
    "labels = ['Positive', 'Negative']\n",
    "sizes = [positive_count, negative_count]\n",
    "colors = ['gold', 'yellowgreen']\n",
    "explode = (0.1, 0) \n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "plt.axis('equal')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
